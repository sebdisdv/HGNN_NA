{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sebdis/ProcessMining/HGNN/HGNN_NA\n",
      "/home/sebdis/ProcessMining/HGNN/HGNN_NA/data/datasets/original/\n",
      "/home/sebdis/ProcessMining/HGNN/HGNN_NA/data/datasets/processed/\n",
      "/home/sebdis/ProcessMining/HGNN/HGNN_NA/data/datasets/graphs/\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from os.path import dirname\n",
    "\n",
    "\n",
    "\n",
    "root_path = dirname(os.getcwd()) + \"/HGNN_NA\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "data_dir = root_path + \"/data/datasets/original/\"\n",
    "data_dir_processed = root_path + \"/data/datasets/processed/\"\n",
    "data_dir_graphs = root_path + \"/data/datasets/graphs/\"\n",
    "\n",
    "print(root_path, data_dir, data_dir_processed, data_dir_graphs, sep=\"\\n\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"BPI_Challenge_2012_W_Complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'org:resource', 'Activity',\n",
    "    \"org:resource:role\"\n",
    "]\n",
    "real_value_columns = [\n",
    "\"time:timestamp\", 'case:AMOUNT_REQ', 'case:REG_DATE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185421/3535436184.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tab_all = pd.read_csv(data_dir_processed+dataset+\"_processed_all.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:resource</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>Activity</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>case:REG_DATE</th>\n",
       "      <th>CaseID</th>\n",
       "      <th>case:AMOUNT_REQ</th>\n",
       "      <th>org:resource:role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10912.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>10.442930</td>\n",
       "      <td>10.421865</td>\n",
       "      <td>173712</td>\n",
       "      <td>10.308953</td>\n",
       "      <td>Role 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11019.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>10.707527</td>\n",
       "      <td>10.421865</td>\n",
       "      <td>173712</td>\n",
       "      <td>10.308953</td>\n",
       "      <td>Role 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11180.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>12.260206</td>\n",
       "      <td>10.421865</td>\n",
       "      <td>173712</td>\n",
       "      <td>10.308953</td>\n",
       "      <td>Role 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11180.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>12.316947</td>\n",
       "      <td>10.421865</td>\n",
       "      <td>173712</td>\n",
       "      <td>10.308953</td>\n",
       "      <td>Role 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10912.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>10.454063</td>\n",
       "      <td>10.398580</td>\n",
       "      <td>173706</td>\n",
       "      <td>9.798127</td>\n",
       "      <td>Role 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  org:resource lifecycle:transition                Activity  time:timestamp  \\\n",
       "0      10912.0             COMPLETE      W_Afhandelen leads       10.442930   \n",
       "1      11019.0             COMPLETE  W_Completeren aanvraag       10.707527   \n",
       "2      11180.0             COMPLETE  W_Completeren aanvraag       12.260206   \n",
       "3      11180.0             COMPLETE  W_Completeren aanvraag       12.316947   \n",
       "4      10912.0             COMPLETE      W_Afhandelen leads       10.454063   \n",
       "\n",
       "   case:REG_DATE  CaseID  case:AMOUNT_REQ org:resource:role  \n",
       "0      10.421865  173712        10.308953            Role 1  \n",
       "1      10.421865  173712        10.308953            Role 1  \n",
       "2      10.421865  173712        10.308953            Role 1  \n",
       "3      10.421865  173712        10.308953            Role 1  \n",
       "4      10.398580  173706         9.798127            Role 1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_all = pd.read_csv(data_dir_processed+dataset+\"_processed_all.csv\")\n",
    "tab_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_dir_graphs \u001b[38;5;241m+\u001b[39m dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_TRAIN_event_prediction_BOOOH.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_dir_graphs \u001b[38;5;241m+\u001b[39m dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_VALID_event_prediction_BOOOH.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     X_valid \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/storage.py:414\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/serialization.py:1114\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/serialization.py:1348\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1347\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1348\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mactive_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/serialization.py:1281\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     obj \u001b[38;5;241m=\u001b[39m cast(Storage, torch\u001b[38;5;241m.\u001b[39mUntypedStorage(nbytes))\n\u001b[1;32m   1280\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1285\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1286\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1287\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/serialization.py:320\u001b[0m, in \u001b[0;36m_cpu_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m backend_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cpu_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(data_dir_graphs + dataset + \"_TRAIN_event_prediction_BOOOH.pkl\", \"rb\") as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open(data_dir_graphs + dataset + \"_VALID_event_prediction_BOOOH.pkl\", \"rb\") as f:\n",
    "    X_valid = pickle.load(f)\n",
    "with open(data_dir_graphs + dataset + \"_TEST_event_prediction_BOOOH.pkl\", \"rb\") as f:\n",
    "    X_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if 'Activity' not in X_train[i].x_dict.keys():\n",
    "        print(\"aaa\")    \n",
    "for i in range(len(X_valid)):\n",
    "    if 'Activity' not in X_valid[i].x_dict.keys():\n",
    "        print(\"bbbb\")   \n",
    "for i in range(len(X_test)):\n",
    "    if 'Activity' not in X_test[i].x_dict.keys():\n",
    "        print(\"cccc\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:int(len(X_train)*0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import ToUndirected, NormalizeFeatures\n",
    "\n",
    "transform = ToUndirected()\n",
    "\n",
    "with torch.no_grad():\n",
    "        for i in range(len(X_train)):\n",
    "                X_train[i] = transform(X_train[i])\n",
    "        for i in range(len(X_valid)):\n",
    "                X_valid[i] = transform(X_valid[i])\n",
    "        for i in range(len(X_test)):\n",
    "                X_test[i] = transform(X_test[i])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_types = set()\n",
    "node_types = set()\n",
    "for i in range(len(X_train)):\n",
    "    n, edge_type = X_train[i].metadata()\n",
    "    for x in n:\n",
    "        node_types.add(x)\n",
    "    for x in edge_type:\n",
    "        edge_types.add(x)\n",
    "for i in range(len(X_valid)):\n",
    "    n, edge_type = X_valid[i].metadata()\n",
    "    for x in n:\n",
    "        node_types.add(x)\n",
    "    for x in edge_type:\n",
    "        edge_types.add(x)\n",
    "for i in range(len(X_test)):\n",
    "    n, edge_type = X_test[i].metadata()\n",
    "    for x in n:\n",
    "        node_types.add(x)\n",
    "    for x in edge_type:\n",
    "        edge_types.add(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_types = list(node_types)\n",
    "edge_types = list(edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(load, key):\n",
    "    weights = []\n",
    "    \n",
    "    cl_train = [0 for _ in tab_all[key].unique()]\n",
    "    \n",
    "    print(cl_train)\n",
    "    \n",
    "    for i,x in enumerate(load):\n",
    "\n",
    "        \n",
    "        classes = x.y[key]\n",
    "\n",
    "        # print(classes)\n",
    "        \n",
    "        for c in list(classes):\n",
    "            try:\n",
    "                cl_train[c] +=1\n",
    "            except KeyError:\n",
    "                cl_train[c] = 1\n",
    "    s = sum(cl_train)\n",
    "    \n",
    "    print(cl_train)\n",
    "    \n",
    "    weights = [s/x if x > 0 else 0 for x in cl_train]\n",
    "\n",
    "    # weights = [0.7,0.7,1,0.7,0.7,0.7,0.7,0.7,0.7,0.7]\n",
    "    weights = torch.tensor(weights, device=device)\n",
    "    print(weights)\n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('org:resource', 'related_to', 'org:resource'): 2,\n",
       " ('Activity', 'followed_by', 'Activity'): 2,\n",
       " ('time:timestamp', 'related_to', 'time:timestamp'): 2,\n",
       " ('org:resource:role', 'related_to', 'org:resource:role'): 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features_dims = {}\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    for k in X_train[i].edge_attr_dict.keys():\n",
    "        edge_features_dims[k] = X_train[i].edge_attr_dict[k].shape[1]\n",
    "for i in range(len(X_valid)):\n",
    "    for k in X_valid[i].edge_attr_dict.keys():\n",
    "        edge_features_dims[k] = X_valid[i].edge_attr_dict[k].shape[1]\n",
    "for i in range(len(X_test)):\n",
    "    for k in X_test[i].edge_attr_dict.keys():\n",
    "        edge_features_dims[k] = X_test[i].edge_attr_dict[k].shape[1]\n",
    "\n",
    "edge_features_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0]\n",
      "[172, 66, 7218, 6261, 12112, 4087]\n",
      "tensor([173.9302, 453.2727,   4.1446,   4.7782,   2.4699,   7.3198],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "act_weights = get_weights(DataLoader(X_train, batch_size=1024, shuffle=False), \"Activity\")\n",
    "# res_roles_weights = get_weights(DataLoader(X_train, batch_size=1024, shuffle=False), \"org:resource:role\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "nan",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_resource_role_map\n\u001b[0;32m----> 3\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mget_resource_role_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtab_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m tab_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg:resource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tab_all\u001b[38;5;241m.\u001b[39mfillna(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m tab_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg:resource:role\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [a[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tab_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg:resource\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues]\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/HGNN_NA/data/utils.py:305\u001b[0m, in \u001b[0;36mget_resource_role_map\u001b[0;34m(log)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_resource_role_map\u001b[39m(log):\n\u001b[0;32m--> 305\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mResourcePoolAnalyser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mresource_table\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/HGNN_NA/data/utils.py:204\u001b[0m, in \u001b[0;36mResourcePoolAnalyser.__init__\u001b[0;34m(self, log, drawing, sim_threshold)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks \u001b[38;5;241m=\u001b[39m {val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())}\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musers \u001b[38;5;241m=\u001b[39m {val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg:resource\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())}\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroles, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscover_roles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/HGNN_NA/data/utils.py:218\u001b[0m, in \u001b[0;36mResourcePoolAnalyser.discover_roles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscover_roles\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    217\u001b[0m     associations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks[x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musers[x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg:resource\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mac_rl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43massociations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     freq_matrix \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroupby(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mac_rl\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    221\u001b[0m                    \u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    222\u001b[0m                    \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m    223\u001b[0m                    \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m'\u001b[39m}))\n\u001b[1;32m    224\u001b[0m     freq_matrix \u001b[38;5;241m=\u001b[39m {x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mac_rl\u001b[39m\u001b[38;5;124m'\u001b[39m]: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m freq_matrix\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/HGNN_NA/data/utils.py:217\u001b[0m, in \u001b[0;36mResourcePoolAnalyser.discover_roles.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscover_roles\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 217\u001b[0m     associations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks[x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m]], \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morg:resource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mac_rl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mapply(associations, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    220\u001b[0m     freq_matrix \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroupby(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mac_rl\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    221\u001b[0m                    \u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    222\u001b[0m                    \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m    223\u001b[0m                    \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m'\u001b[39m}))\n",
      "\u001b[0;31mKeyError\u001b[0m: nan"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_all[\"org:resource\"] = [str(x) for x in tab_all[\"org:resource\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10912.0': 'Role 1',\n",
       " '11019.0': 'Role 1',\n",
       " '11180.0': 'Role 1',\n",
       " 'NAN': 'Role 1',\n",
       " '10982.0': 'Role 1',\n",
       " '11002.0': 'Role 1',\n",
       " '11049.0': 'Role 1',\n",
       " '10629.0': 'Role 2',\n",
       " '11122.0': 'Role 1',\n",
       " '10913.0': 'Role 1',\n",
       " '10889.0': 'Role 1',\n",
       " '10972.0': 'Role 2',\n",
       " '11121.0': 'Role 1',\n",
       " '10939.0': 'Role 1',\n",
       " '10609.0': 'Role 2',\n",
       " '11009.0': 'Role 1',\n",
       " '11201.0': 'Role 1',\n",
       " '11119.0': 'Role 1',\n",
       " '10861.0': 'Role 1',\n",
       " '11203.0': 'Role 1',\n",
       " '11181.0': 'Role 1',\n",
       " '11189.0': 'Role 1',\n",
       " '10809.0': 'Role 2',\n",
       " '10899.0': 'Role 1',\n",
       " '10138.0': 'Role 2',\n",
       " '11000.0': 'Role 1',\n",
       " '10863.0': 'Role 1',\n",
       " '11169.0': 'Role 1',\n",
       " '11179.0': 'Role 1',\n",
       " '11001.0': 'Role 1',\n",
       " '10228.0': 'Role 1',\n",
       " '10909.0': 'Role 1',\n",
       " '10789.0': 'Role 1',\n",
       " '10881.0': 'Role 1',\n",
       " '10910.0': 'Role 1',\n",
       " '10929.0': 'Role 1',\n",
       " '10931.0': 'Role 1',\n",
       " '11259.0': 'Role 1',\n",
       " '10188.0': 'Role 3',\n",
       " '10779.0': 'Role 1',\n",
       " '10914.0': 'Role 1',\n",
       " '11339.0': 'Role 2',\n",
       " '10933.0': 'Role 1',\n",
       " '11079.0': 'Role 1',\n",
       " '10932.0': 'Role 1',\n",
       " '10935.0': 'Role 1',\n",
       " '11254.0': 'Role 1',\n",
       " '11003.0': 'Role 1',\n",
       " '11269.0': 'Role 1',\n",
       " '10821.0': 'Role 2',\n",
       " '11289.0': 'Role 2',\n",
       " '10125.0': 'Role 2',\n",
       " '112.0': 'Role 1',\n",
       " '11299.0': 'Role 1',\n",
       " '10124.0': 'Role 1',\n",
       " '11309.0': 'Role 1',\n",
       " '11300.0': 'Role 1',\n",
       " '11302.0': 'Role 1',\n",
       " '11319.0': 'Role 1',\n",
       " '11304.0': 'Role 3'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import utils\n",
    "\n",
    "one_hot_resource = utils.get_one_hot_encoder(tab_all, \"org:resource\")\n",
    "one_hot_resource_role = utils.get_one_hot_encoder(tab_all, \"org:resource:role\")\n",
    "resources = tab_all[\"org:resource\"]\n",
    "resources_role = tab_all[\"org:resource:role\"]\n",
    "map_resource_to_role = {}\n",
    "for i in range(len(resources)):\n",
    "    map_resource_to_role[resources[i]] = resources_role[i]\n",
    "\n",
    "map_resource_to_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10912.0': 18,\n",
       " '11019.0': 34,\n",
       " '11180.0': 42,\n",
       " 'NAN': 59,\n",
       " '10982.0': 28,\n",
       " '11002.0': 31,\n",
       " '11049.0': 35,\n",
       " '10629.0': 6,\n",
       " '11122.0': 39,\n",
       " '10913.0': 19,\n",
       " '10889.0': 14,\n",
       " '10972.0': 27,\n",
       " '11121.0': 38,\n",
       " '10939.0': 26,\n",
       " '10609.0': 5,\n",
       " '11009.0': 33,\n",
       " '11201.0': 46,\n",
       " '11119.0': 37,\n",
       " '10861.0': 11,\n",
       " '11203.0': 47,\n",
       " '11181.0': 43,\n",
       " '11189.0': 44,\n",
       " '10809.0': 9,\n",
       " '10899.0': 15,\n",
       " '10138.0': 2,\n",
       " '11000.0': 29,\n",
       " '10863.0': 12,\n",
       " '11169.0': 40,\n",
       " '11179.0': 41,\n",
       " '11001.0': 30,\n",
       " '10228.0': 4,\n",
       " '10909.0': 16,\n",
       " '10789.0': 8,\n",
       " '10881.0': 13,\n",
       " '10910.0': 17,\n",
       " '10929.0': 21,\n",
       " '10931.0': 22,\n",
       " '11259.0': 49,\n",
       " '10188.0': 3,\n",
       " '10779.0': 7,\n",
       " '10914.0': 20,\n",
       " '11339.0': 58,\n",
       " '10933.0': 24,\n",
       " '11079.0': 36,\n",
       " '10932.0': 23,\n",
       " '10935.0': 25,\n",
       " '11254.0': 48,\n",
       " '11003.0': 32,\n",
       " '11269.0': 50,\n",
       " '10821.0': 10,\n",
       " '11289.0': 51,\n",
       " '10125.0': 1,\n",
       " '112.0': 45,\n",
       " '11299.0': 52,\n",
       " '10124.0': 0,\n",
       " '11309.0': 56,\n",
       " '11300.0': 53,\n",
       " '11302.0': 54,\n",
       " '11319.0': 57,\n",
       " '11304.0': 55}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_resource_index = {}\n",
    "for x in tab_all[\"org:resource\"].unique():\n",
    "    map_resource_index[x] = torch.argmax(torch.tensor(utils.get_one_hot_encodings(one_hot_resource, np.array([x])))).item()\n",
    "map_resource_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Role 1': 0, 'Role 2': 1, 'Role 3': 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_resource_role_index = {}\n",
    "for x in tab_all[\"org:resource:role\"].unique():\n",
    "    map_resource_role_index[x] = torch.argmax(torch.tensor(utils.get_one_hot_encodings(one_hot_resource_role, np.array([x])))).item()\n",
    "map_resource_role_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{18: 0,\n",
       " 34: 0,\n",
       " 42: 0,\n",
       " 59: 0,\n",
       " 28: 0,\n",
       " 31: 0,\n",
       " 35: 0,\n",
       " 6: 1,\n",
       " 39: 0,\n",
       " 19: 0,\n",
       " 14: 0,\n",
       " 27: 1,\n",
       " 38: 0,\n",
       " 26: 0,\n",
       " 5: 1,\n",
       " 33: 0,\n",
       " 46: 0,\n",
       " 37: 0,\n",
       " 11: 0,\n",
       " 47: 0,\n",
       " 43: 0,\n",
       " 44: 0,\n",
       " 9: 1,\n",
       " 15: 0,\n",
       " 2: 1,\n",
       " 29: 0,\n",
       " 12: 0,\n",
       " 40: 0,\n",
       " 41: 0,\n",
       " 30: 0,\n",
       " 4: 0,\n",
       " 16: 0,\n",
       " 8: 0,\n",
       " 13: 0,\n",
       " 17: 0,\n",
       " 21: 0,\n",
       " 22: 0,\n",
       " 49: 0,\n",
       " 3: 2,\n",
       " 7: 0,\n",
       " 20: 0,\n",
       " 58: 1,\n",
       " 24: 0,\n",
       " 36: 0,\n",
       " 23: 0,\n",
       " 25: 0,\n",
       " 48: 0,\n",
       " 32: 0,\n",
       " 50: 0,\n",
       " 10: 1,\n",
       " 51: 1,\n",
       " 1: 1,\n",
       " 45: 0,\n",
       " 52: 0,\n",
       " 0: 0,\n",
       " 56: 0,\n",
       " 53: 0,\n",
       " 54: 0,\n",
       " 57: 0,\n",
       " 55: 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_resource_to_role = { map_resource_index[k] : map_resource_role_index[v] for k,v in map_resource_to_role.items()}\n",
    "map_resource_to_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.service.managed_loop import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import (\n",
    "    HeteroConv,\n",
    "    global_mean_pool,\n",
    "    GATv2Conv\n",
    ")\n",
    "from torch.nn import (\n",
    "    ModuleList,\n",
    "    Module,\n",
    "    Linear\n",
    "  )\n",
    "from typing_extensions import Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class HGNN(Module):\n",
    "\n",
    "    def __init__(self, output_cat, output_real,nodes_relations, relations_with_features, parameters) -> Self:  # type: ignore\n",
    "        super().__init__()\n",
    "\n",
    "        # List of convolutional layers\n",
    "        \n",
    "        hid = parameters[\"hid\"]\n",
    "        layers = parameters[\"layers\"]\n",
    "        aggregation = parameters[\"aggregation\"]\n",
    "        n_heads = parameters[\"heads\"]\n",
    "        \n",
    "        self.output_cat = output_cat\n",
    "        self.output_real = output_real\n",
    "        \n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(layers):\n",
    "            conv = HeteroConv(\n",
    "                {\n",
    "                    relation: (\n",
    "                        GATv2Conv((-1,-1), add_self_loops=False, out_channels=hid, heads=n_heads, concat=False)\n",
    "                        if relation not in relations_with_features\n",
    "                        else GATv2Conv((-1,-1), add_self_loops=False, out_channels=hid,heads=n_heads, edge_dim=relations_with_features[relation], concat=False)\n",
    "                    )\n",
    "                    for relation in nodes_relations\n",
    "                },\n",
    "                aggr=aggregation,\n",
    "            )\n",
    "\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.FC = {}\n",
    "        \n",
    "        for k in output_cat:\n",
    "            self.FC[k] = Linear(hid, output_cat[k], device=device)\n",
    "        for k in output_real:\n",
    "            self.FC[k] = Linear(hid, 1, device=device)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        #print(f\"inside: {batch.x_dict.keys()}\")\n",
    "        \n",
    "       \n",
    "        \n",
    "        for i in range(len(self.convs)):\n",
    "            batch.x_dict = self.convs[i]( \n",
    "                batch.x_dict, batch.edge_index_dict, batch.edge_attr_dict\n",
    "            )\n",
    "            #print(f\"inside2_{i}: {batch.x_dict.keys()}\")\n",
    "            batch.x_dict = {key: x.relu() for key, x in batch.x_dict.items()}\n",
    "        \n",
    "        #print(batch.x_dict)\n",
    "        #batch.x_dict = self.convs[0]( \n",
    "        #      batch.x_dict, batch.edge_index_dict, batch.edge_attr_dict\n",
    "        #)\n",
    "        #batch.x_dict = {key: x.relu() for key, x in batch.x_dict.items()}\n",
    "        #print(f\"inside0: {batch.x_dict.keys()}\")\n",
    "        #print(batch.x_dict)\n",
    "        \n",
    "        #batch.x_dict = self.convs[1]( \n",
    "        #      batch.x_dict, batch.edge_index_dict, batch.edge_attr_dict\n",
    "        #)\n",
    "        #batch.x_dict = {key: x.relu() for key, x in batch.x_dict.items()}\n",
    "        #print(f\"inside1: {batch.x_dict.keys()}\")\n",
    "        #print(batch.x_dict)\n",
    "        \n",
    "        \n",
    "\n",
    "        output = {}\n",
    "        try:\n",
    "            for k in self.output_cat:\n",
    "                output[k] = global_mean_pool(batch.x_dict[k], batch[k].batch)\n",
    "                output[k] = self.FC[k](output[k])\n",
    "            for k in self.output_real:\n",
    "                output[k] = global_mean_pool(batch.x_dict[k], batch[k].batch)\n",
    "                output[k] = self.FC[k](output[k]).reshape(1,-1)[0]\n",
    "        except KeyError:\n",
    "            print(\"error\")\n",
    "            return None   \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import multiclass_accuracy, multiclass_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def train_hgnn(config, output_cat, output_real, epochs=15):\n",
    "    print(config)\n",
    "\n",
    "    net = HGNN(\n",
    "        parameters=config,\n",
    "        output_cat=output_cat,\n",
    "        output_real=output_real,\n",
    "        nodes_relations=edge_types,\n",
    "        relations_with_features=edge_features_dims,\n",
    "    )\n",
    "    net = net.to(device)\n",
    "\n",
    "    losses = {}\n",
    "\n",
    "    for k in output_cat:\n",
    "        losses[k] = (\n",
    "            nn.CrossEntropyLoss()\n",
    "            if k != \"Activity\"\n",
    "            else nn.CrossEntropyLoss(act_weights)\n",
    "        )\n",
    "    for k in output_real:\n",
    "        losses[k] = nn.L1Loss()\n",
    "\n",
    "    train_loader = DataLoader(X_train, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    valid_loader = DataLoader(X_valid, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = 0\n",
    "    patience = 4\n",
    "    pat_count = 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"Epoch: {epoch}\\n\")\n",
    "\n",
    "        net.train()\n",
    "        \n",
    "        #for i,x in enumerate(train_loader):\n",
    "        #    if \"Activity\" not in x.x_dict.keys():\n",
    "        #        print(\"aa\")\n",
    "        \n",
    "        for _, x in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            labels = x.y\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #print(\"Keys in x_dict:\", x.x_dict.keys())\n",
    "        \n",
    "            outputs = net(x)\n",
    "            \n",
    "            if outputs != None:\n",
    "                \n",
    "\n",
    "                losses_step = {k: losses[k](outputs[k], labels[k]) for k in losses}\n",
    "\n",
    "                total_loss = 0\n",
    "                for k in losses_step:\n",
    "                    total_loss += losses_step[k]\n",
    "\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "\n",
    "        predictions_categorical = {k: [] for k in output_cat}\n",
    "        target_categorical = {k: [] for k in output_cat}\n",
    "\n",
    "        avg_MAE = {k: [] for k in output_real}\n",
    "\n",
    "        running_total_loss = []\n",
    "        \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, x in enumerate(valid_loader):\n",
    "                x = x.to(device)\n",
    "\n",
    "                labels = x.y\n",
    "\n",
    "                outputs = net(x)\n",
    "\n",
    "                if outputs != None:\n",
    "                \n",
    "                    losses_step = {k: losses[k](outputs[k], labels[k]) for k in losses}\n",
    "    \n",
    "                    running_total_loss.append(sum(list(losses_step.values())))\n",
    "    \n",
    "                    for k in output_cat:\n",
    "                        predictions_categorical[k].append(\n",
    "                            torch.argmax(torch.softmax(outputs[k], dim=1), 1)\n",
    "                        )\n",
    "                        target_categorical[k].append(labels[k])\n",
    "    \n",
    "                    for k in output_real:\n",
    "                        avg_MAE[k].append(losses_step[k])\n",
    "    \n",
    "        for k in predictions_categorical:\n",
    "            predictions_categorical[k] = torch.cat(predictions_categorical[k])\n",
    "            target_categorical[k] = torch.cat(target_categorical[k])\n",
    "\n",
    "        macro_f1_activity = multiclass_f1_score(\n",
    "            predictions_categorical[\"Activity\"],\n",
    "            target_categorical[\"Activity\"],\n",
    "            num_classes=output_cat[\"Activity\"],\n",
    "            average=\"macro\",\n",
    "        )\n",
    "\n",
    "        accuracy = {\n",
    "            k: multiclass_accuracy(\n",
    "                predictions_categorical[k],\n",
    "                target_categorical[k],\n",
    "                num_classes=output_cat[k],\n",
    "            )\n",
    "            for k in output_cat\n",
    "        }\n",
    "\n",
    "        avg_MAE = {k: sum(avg_MAE[k]) / len(avg_MAE[k]) for k in avg_MAE}\n",
    "\n",
    "        val_loss = sum(running_total_loss) / len(running_total_loss)\n",
    "\n",
    "        print(f\"\\nVALIDATION\")\n",
    "        for k in accuracy:\n",
    "            (\n",
    "                print(\"{}: acc {:.4f}\".format(k, accuracy[k]))\n",
    "                if k != \"Activity\"\n",
    "                else print(\n",
    "                    \"{}: acc {:.4f} macroF1 {:.4f}\".format(\n",
    "                        k, accuracy[k], macro_f1_activity.item()\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        for k in avg_MAE:\n",
    "            print(\"{}: MAE {:.4f}\".format(k, avg_MAE[k]))\n",
    "        print(\"TOTAL_LOSS: {:.4f}\".format(val_loss))\n",
    "        print(\"epoch time {}s\\n\".format(time.time() - start_time))\n",
    "\n",
    "        if epoch == 0:\n",
    "            best_model = deepcopy(net)\n",
    "            best_loss = val_loss\n",
    "        else:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = deepcopy(net)\n",
    "                pat_count = 0\n",
    "                print(\"new best model found\")\n",
    "            if pat_count == patience:\n",
    "                print(\n",
    "                    \"Validation performance didn't improve for {} epochs. Training stops.\".format(\n",
    "                        pat_count\n",
    "                    )\n",
    "                )\n",
    "                return best_model\n",
    "        pat_count += 1\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hgnn(net, output_cat, output_real):\n",
    "    test_loader = DataLoader(X_train, batch_size=128, shuffle=False)\n",
    "    \n",
    "    losses = {}\n",
    "    \n",
    "    for k in output_cat:\n",
    "        losses[k] = (\n",
    "            nn.CrossEntropyLoss()\n",
    "            if k != \"Activity\"\n",
    "            else nn.CrossEntropyLoss(act_weights)\n",
    "        )\n",
    "    for k in output_real:\n",
    "        losses[k] = nn.L1Loss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    predictions_categorical = {k: [] for k in output_cat}\n",
    "    target_categorical = {k: [] for k in output_cat}\n",
    "\n",
    "    avg_MAE = {k : [] for k in output_real}\n",
    "    \n",
    "    total_loss = []\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, x in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            labels = x.y\n",
    "            \n",
    "            outputs = net(x)\n",
    "            \n",
    "     \n",
    "            losses_step = {k: losses[k](outputs[k], labels[k]).item() for k in losses}\n",
    "            total_loss.append(sum(list(losses_step.values())))\n",
    "            \n",
    "            for k in output_cat:\n",
    "                    predictions_categorical[k].append(\n",
    "                        torch.argmax(torch.softmax(outputs[k], dim=1), 1)\n",
    "                    )\n",
    "                    target_categorical[k].append(labels[k])\n",
    "            \n",
    "            \n",
    "            for k in output_real:\n",
    "                    avg_MAE[k].append(losses_step[k])\n",
    "                    \n",
    "    for k in predictions_categorical:\n",
    "            predictions_categorical[k] = torch.cat(predictions_categorical[k])\n",
    "            target_categorical[k] = torch.cat(target_categorical[k])\n",
    "               \n",
    "            \n",
    "    macro_f1_activity = multiclass_f1_score(\n",
    "            predictions_categorical[\"Activity\"],\n",
    "            target_categorical[\"Activity\"],\n",
    "            num_classes=output_cat[\"Activity\"],\n",
    "            average=\"macro\",\n",
    "        )\n",
    "            \n",
    "    accuracy = {\n",
    "            k: multiclass_accuracy(\n",
    "                predictions_categorical[k],\n",
    "                target_categorical[k],\n",
    "                num_classes=output_cat[k],\n",
    "            )\n",
    "            for k in output_cat\n",
    "        }\n",
    "    \n",
    "    resource_to_role_acc = multiclass_accuracy(\n",
    "        torch.tensor([map_resource_to_role[x.item()] for x in predictions_categorical[\"org:resource\"]], device=device),\n",
    "        target_categorical[\"org:resource:role\"],\n",
    "        num_classes=output_cat[\"org:resource:role\"]\n",
    "    )\n",
    "    \n",
    "    avg_MAE = {k : sum(avg_MAE[k]) / len(avg_MAE[k]) for k in avg_MAE}\n",
    "    \n",
    "    \n",
    "    Average_total_loss = sum(total_loss) / len(total_loss)\n",
    "    \n",
    "    res = {f\"{k}_acc\" : accuracy[k].item() for k in accuracy} | {\"Resource_to_role_acc\" : resource_to_role_acc.item()} | {\"MacroF1Act\" : macro_f1_activity.item()} | {f\"{k}_mae\" : avg_MAE[k] for k in avg_MAE} | {\"AVG_total_loss\" : Average_total_loss} \n",
    "    \n",
    "    print(res)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(X_train, batch_size=256, shuffle=True)\n",
    "for i,x in enumerate(train_loader):\n",
    "    if \"Activity\" not in x.x_dict.keys():\n",
    "        print(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'org:resource': ['10912.0',\n",
       "  '11019.0',\n",
       "  '11180.0',\n",
       "  'NAN',\n",
       "  '10982.0',\n",
       "  '11002.0',\n",
       "  '11049.0',\n",
       "  '10629.0',\n",
       "  '11122.0',\n",
       "  '10913.0',\n",
       "  '10889.0',\n",
       "  '10972.0',\n",
       "  '11121.0',\n",
       "  '10939.0',\n",
       "  '10609.0',\n",
       "  '11009.0',\n",
       "  '11201.0',\n",
       "  '11119.0',\n",
       "  '10861.0',\n",
       "  '11203.0',\n",
       "  '11181.0',\n",
       "  '11189.0',\n",
       "  '10809.0',\n",
       "  '10899.0',\n",
       "  '10138.0',\n",
       "  '11000.0',\n",
       "  '10863.0',\n",
       "  '11169.0',\n",
       "  '11179.0',\n",
       "  '11001.0',\n",
       "  '10228.0',\n",
       "  '10909.0',\n",
       "  '10789.0',\n",
       "  '10881.0',\n",
       "  '10910.0',\n",
       "  '10929.0',\n",
       "  '10931.0',\n",
       "  '11259.0',\n",
       "  '10188.0',\n",
       "  '10779.0',\n",
       "  '10914.0',\n",
       "  '11339.0',\n",
       "  '10933.0',\n",
       "  '11079.0',\n",
       "  '10932.0',\n",
       "  '10935.0',\n",
       "  '11254.0',\n",
       "  '11003.0',\n",
       "  '11269.0',\n",
       "  '10821.0',\n",
       "  '11289.0',\n",
       "  '10125.0',\n",
       "  '112.0',\n",
       "  '11299.0',\n",
       "  '10124.0',\n",
       "  '11309.0',\n",
       "  '11300.0',\n",
       "  '11302.0',\n",
       "  '11319.0',\n",
       "  '11304.0'],\n",
       " 'Activity': ['W_Afhandelen leads',\n",
       "  'W_Completeren aanvraag',\n",
       "  'W_Nabellen offertes',\n",
       "  'W_Valideren aanvraag',\n",
       "  'W_Nabellen incomplete dossiers',\n",
       "  'W_Beoordelen fraude'],\n",
       " 'org:resource:role': ['Role 1', 'Role 2', 'Role 3']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique = {k : list(tab_all[k].unique()) for k in categorical_columns}\n",
    "list_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'org:resource': 60, 'Activity': 6, 'org:resource:role': 3}\n",
      "['time:timestamp']\n"
     ]
    }
   ],
   "source": [
    "outputcat = {k : len(list_unique[k]) if len(list_unique[k]) > 1 else None for k in list_unique}\n",
    "outputcat = {k : v for k,v in outputcat.items() if v != None}\n",
    "outputreal = ['time:timestamp']\n",
    "print(outputcat)\n",
    "print(outputreal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(config):\n",
    "    trained_net = train_hgnn(config, output_cat=outputcat, output_real=outputreal, epochs = 1)\n",
    "    return test_hgnn(trained_net, output_cat=outputcat, output_real=outputreal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\" : 0.001903641203982847,\n",
    "    \"heads\" : 1,\n",
    "    \"hid\" : 128, \n",
    "    \"layers\" : 2,\n",
    "    \"batch_size\" : 2048,\n",
    "    \"aggregation\" : \"max\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001903641203982847, 'heads': 1, 'hid': 128, 'layers': 2, 'batch_size': 2048, 'aggregation': 'max'}\n",
      "Epoch: 0\n",
      "\n",
      "\n",
      "VALIDATION\n",
      "org:resource: acc 0.0431\n",
      "Activity: acc 0.3604 macroF1 0.0883\n",
      "org:resource:role: acc 0.8282\n",
      "time:timestamp: MAE 1.9208\n",
      "TOTAL_LOSS: 8.2149\n",
      "epoch time 32.81581926345825s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m, in \u001b[0;36mtrain_evaluate\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_evaluate\u001b[39m(config):\n\u001b[1;32m      2\u001b[0m     trained_net \u001b[38;5;241m=\u001b[39m train_hgnn(config, output_cat\u001b[38;5;241m=\u001b[39moutputcat, output_real\u001b[38;5;241m=\u001b[39moutputreal, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtest_hgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_cat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_real\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputreal\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 28\u001b[0m, in \u001b[0;36mtest_hgnn\u001b[0;34m(net, output_cat, output_real)\u001b[0m\n\u001b[1;32m     26\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[1;32m     29\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m         labels \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39my\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/collate.py:109\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/collate.py:169\u001b[0m, in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    167\u001b[0m slices \u001b[38;5;241m=\u001b[39m cumsum(sizes)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment:\n\u001b[0;32m--> 169\u001b[0m     incs \u001b[38;5;241m=\u001b[39m \u001b[43mget_incs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(incs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    171\u001b[0m         values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    172\u001b[0m             value \u001b[38;5;241m+\u001b[39m inc\u001b[38;5;241m.\u001b[39mto(value\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value, inc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, incs)\n\u001b[1;32m    174\u001b[0m         ]\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/collate.py:327\u001b[0m, in \u001b[0;36mget_incs\u001b[0;34m(key, values, data_list, stores)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_incs\u001b[39m(key, values: List[Any], data_list: List[BaseData],\n\u001b[1;32m    326\u001b[0m              stores: List[BaseStorage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 327\u001b[0m     repeats \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    328\u001b[0m         data\u001b[38;5;241m.\u001b[39m__inc__(key, value, store)\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value, data, store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, data_list, stores)\n\u001b[1;32m    330\u001b[0m     ]\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repeats[\u001b[38;5;241m0\u001b[39m], Tensor):\n\u001b[1;32m    332\u001b[0m         repeats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(repeats, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/collate.py:328\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_incs\u001b[39m(key, values: List[Any], data_list: List[BaseData],\n\u001b[1;32m    326\u001b[0m              stores: List[BaseStorage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    327\u001b[0m     repeats \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 328\u001b[0m         \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inc__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value, data, store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, data_list, stores)\n\u001b[1;32m    330\u001b[0m     ]\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repeats[\u001b[38;5;241m0\u001b[39m], Tensor):\n\u001b[1;32m    332\u001b[0m         repeats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(repeats, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/hetero_data.py:354\u001b[0m, in \u001b[0;36mHeteroData.__inc__\u001b[0;34m(self, key, value, store, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(value\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(store, EdgeStorage) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/storage.py:630\u001b[0m, in \u001b[0;36mEdgeStorage.size\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msize\u001b[39m(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    628\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[Optional[\u001b[38;5;28mint\u001b[39m], Optional[\u001b[38;5;28mint\u001b[39m]], Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m without explicit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_key\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    634\u001b[0m     size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent()[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mnum_nodes,\n\u001b[1;32m    635\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent()[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mnum_nodes)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/storage.py:550\u001b[0m, in \u001b[0;36mEdgeStorage._key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEdgeStorage\u001b[39;00m(BaseStorage):\n\u001b[1;32m    534\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"A storage for edge-level information.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \n\u001b[1;32m    536\u001b[0m \u001b[38;5;124;03m    We support multiple ways to store edge connectivity in a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m      indices are sorted based on target nodes.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_key\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EdgeType:\n\u001b[1;32m    552\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_key\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_evaluate(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 01-14 20:39:07] ax.service.utils.instantiation: Choice parameter hid contains only one value, converting to a fixed parameter instead.\n",
      "[INFO 01-14 20:39:07] ax.service.utils.instantiation: Choice parameter layers contains only one value, converting to a fixed parameter instead.\n",
      "[INFO 01-14 20:39:07] ax.service.utils.instantiation: Choice parameter aggregation contains only one value, converting to a fixed parameter instead.\n",
      "[INFO 01-14 20:39:07] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[FixedParameter(name='hid', parameter_type=INT, value=128), FixedParameter(name='layers', parameter_type=INT, value=2), RangeParameter(name='lr', parameter_type=FLOAT, range=[0.0001, 0.1], log_scale=True), ChoiceParameter(name='batch_size', parameter_type=INT, values=[128, 256, 512], is_ordered=True, sort_values=False), ChoiceParameter(name='heads', parameter_type=INT, values=[1, 2], is_ordered=True, sort_values=False), FixedParameter(name='aggregation', parameter_type=STRING, value='max')], parameter_constraints=[]).\n",
      "[INFO 01-14 20:39:07] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 01-14 20:39:07] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=3 num_trials=None use_batch_trials=False\n",
      "[INFO 01-14 20:39:07] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=6\n",
      "[INFO 01-14 20:39:07] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=6\n",
      "[INFO 01-14 20:39:07] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 01-14 20:39:07] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 6 trials, BoTorch for subsequent trials]). Iterations after 6 will take longer to generate due to model-fitting.\n",
      "[INFO 01-14 20:39:07] ax.service.managed_loop: Started full optimization with 20 steps.\n",
      "[INFO 01-14 20:39:07] ax.service.managed_loop: Running optimization trial 1...\n",
      "/home/sebdis/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0001865322718407359, 'batch_size': 256, 'heads': 1, 'hid': 128, 'layers': 2, 'aggregation': 'max'}\n",
      "Epoch: 0\n",
      "\n",
      "\n",
      "VALIDATION\n",
      "org:resource: acc 0.0431\n",
      "Activity: acc 0.5417 macroF1 0.2359\n",
      "org:resource:role: acc 0.8282\n",
      "time:timestamp: MAE 3.7173\n",
      "TOTAL_LOSS: 10.0132\n",
      "epoch time 39.306875228881836s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_parameters, values, experiment, model \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_ordered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#{\"name\": \"layers\", \"type\": \"choice\", \"values\": [2, 3, 4, 5], \"value_type\": \"int\", \"is_ordered\" : True, \"sort_values\":False},\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_ordered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbounds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_ordered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_ordered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#{\"name\": \"heads\", \"type\": \"choice\", \"values\": [1], \"value_type\": \"int\", \"is_ordered\" : True,\"sort_values\":False},\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#{\"name\": \"aggregation\", \"type\" : \"choice\", \"values\" :[\"sum\", \"mean\", \"max\"], \"value_type\" : \"str\"}\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maggregation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m     \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_evaluate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAVG_total_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43marms_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_parameters)\n\u001b[1;32m     25\u001b[0m means, covariances \u001b[38;5;241m=\u001b[39m values\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/ax/service/managed_loop.py:307\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(parameters, evaluation_function, experiment_name, objective_name, minimize, parameter_constraints, outcome_constraints, total_trials, arms_per_trial, random_seed, generation_strategy)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct and run a full optimization loop.\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m loop \u001b[38;5;241m=\u001b[39m OptimizationLoop\u001b[38;5;241m.\u001b[39mwith_evaluation_function(\n\u001b[1;32m    295\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    296\u001b[0m     objective_name\u001b[38;5;241m=\u001b[39mobjective_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     generation_strategy\u001b[38;5;241m=\u001b[39mgeneration_strategy,\n\u001b[1;32m    306\u001b[0m )\n\u001b[0;32m--> 307\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m parameterization, values \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mget_best_point()\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parameterization, values, loop\u001b[38;5;241m.\u001b[39mexperiment, loop\u001b[38;5;241m.\u001b[39mget_current_model()\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/ax/service/managed_loop.py:238\u001b[0m, in \u001b[0;36mOptimizationLoop.full_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m SearchSpaceExhausted \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    240\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    241\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped optimization as the search space is exhaused. Message \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom generation strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         )\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/ax/utils/common/executils.py:167\u001b[0m, in \u001b[0;36mretry_on_exception.<locals>.func_wrapper.<locals>.actual_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m             wait_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    164\u001b[0m                 MAX_WAIT_SECONDS, initial_wait_seconds \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    165\u001b[0m             )\n\u001b[1;32m    166\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(wait_interval)\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# If we are here, it means the retries were finished but\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The error was suppressed. Hence return the default value provided.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default_return_on_suppression\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/ax/service/managed_loop.py:215\u001b[0m, in \u001b[0;36mOptimizationLoop.run_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_trial()\n\u001b[1;32m    213\u001b[0m trial\u001b[38;5;241m.\u001b[39mmark_running(no_runner_required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    214\u001b[0m _, data \u001b[38;5;241m=\u001b[39m data_and_evaluations_from_raw_data(\n\u001b[0;32m--> 215\u001b[0m     raw_data\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    216\u001b[0m         arm\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_evaluation_function(arm\u001b[38;5;241m.\u001b[39mparameters, weight)\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arm, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights_by_arm(trial)\n\u001b[1;32m    218\u001b[0m     },\n\u001b[1;32m    219\u001b[0m     trial_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_trial,\n\u001b[1;32m    220\u001b[0m     sample_sizes\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    221\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mdefault_data_type,\n\u001b[1;32m    222\u001b[0m     metric_names\u001b[38;5;241m=\u001b[39mnot_none(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39moptimization_config\n\u001b[1;32m    224\u001b[0m     )\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mmetric_names,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mattach_data(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    228\u001b[0m trial\u001b[38;5;241m.\u001b[39mmark_completed()\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/ax/service/managed_loop.py:216\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    211\u001b[0m trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_trial()\n\u001b[1;32m    213\u001b[0m trial\u001b[38;5;241m.\u001b[39mmark_running(no_runner_required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    214\u001b[0m _, data \u001b[38;5;241m=\u001b[39m data_and_evaluations_from_raw_data(\n\u001b[1;32m    215\u001b[0m     raw_data\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m--> 216\u001b[0m         arm\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_evaluation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43marm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arm, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights_by_arm(trial)\n\u001b[1;32m    218\u001b[0m     },\n\u001b[1;32m    219\u001b[0m     trial_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_trial,\n\u001b[1;32m    220\u001b[0m     sample_sizes\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    221\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mdefault_data_type,\n\u001b[1;32m    222\u001b[0m     metric_names\u001b[38;5;241m=\u001b[39mnot_none(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39moptimization_config\n\u001b[1;32m    224\u001b[0m     )\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mmetric_names,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mattach_data(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    228\u001b[0m trial\u001b[38;5;241m.\u001b[39mmark_completed()\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/ax/service/managed_loop.py:154\u001b[0m, in \u001b[0;36mOptimizationLoop._call_evaluation_function\u001b[0;34m(self, parameterization, weight)\u001b[0m\n\u001b[1;32m    151\u001b[0m num_evaluation_function_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_evaluation_function_params \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# pyre-ignore [20]: Can't run instance checks on subscripted generics.\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     evaluation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameterization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_evaluation_function_params \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# pyre-ignore [19]: Can't run instance checks on subscripted generics.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     evaluation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_function(parameterization, weight)\n",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m, in \u001b[0;36mtrain_evaluate\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_evaluate\u001b[39m(config):\n\u001b[1;32m      2\u001b[0m     trained_net \u001b[38;5;241m=\u001b[39m train_hgnn(config, output_cat\u001b[38;5;241m=\u001b[39moutputcat, output_real\u001b[38;5;241m=\u001b[39moutputreal, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtest_hgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_cat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_real\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputreal\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 28\u001b[0m, in \u001b[0;36mtest_hgnn\u001b[0;34m(net, output_cat, output_real)\u001b[0m\n\u001b[1;32m     26\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[1;32m     29\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m         labels \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39my\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/collate.py:143\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (add_batch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stores[\u001b[38;5;241m0\u001b[39m], NodeStorage)\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m stores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcan_infer_num_nodes):\n\u001b[1;32m    142\u001b[0m         repeats \u001b[38;5;241m=\u001b[39m [store\u001b[38;5;241m.\u001b[39mnum_nodes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m stores]\n\u001b[0;32m--> 143\u001b[0m         out_store\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m         out_store\u001b[38;5;241m.\u001b[39mptr \u001b[38;5;241m=\u001b[39m cumsum(torch\u001b[38;5;241m.\u001b[39mtensor(repeats, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, slice_dict, inc_dict\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/collate.py:321\u001b[0m, in \u001b[0;36mrepeat_interleave\u001b[0;34m(repeats, device)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat_interleave\u001b[39m(\n\u001b[1;32m    318\u001b[0m     repeats: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    319\u001b[0m     device: Optional[torch\u001b[38;5;241m.\u001b[39mdevice] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 321\u001b[0m     outs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfull((n, ), i, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(repeats)]\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/ProcessMining/HGNN/hgnn_env/lib/python3.10/site-packages/torch_geometric/data/collate.py:321\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat_interleave\u001b[39m(\n\u001b[1;32m    318\u001b[0m     repeats: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    319\u001b[0m     device: Optional[torch\u001b[38;5;241m.\u001b[39mdevice] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 321\u001b[0m     outs \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(repeats)]\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"hid\", \"type\": \"choice\", \"values\": [128], \"value_type\": \"int\", \"is_ordered\" : True,\"sort_values\":False},\n",
    "        #{\"name\": \"layers\", \"type\": \"choice\", \"values\": [2, 3, 4, 5], \"value_type\": \"int\", \"is_ordered\" : True, \"sort_values\":False},\n",
    "        {\"name\": \"layers\", \"type\": \"choice\", \"values\": [2], \"value_type\": \"int\", \"is_ordered\" : True, \"sort_values\":False},\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-4, 1e-1], \"value_type\": \"float\", \"log_scale\": True},\n",
    "        {\"name\": \"batch_size\", \"type\": \"choice\", \"values\": [128, 256, 512], \"value_type\": \"int\", \"is_ordered\" : True,\"sort_values\":False}, \n",
    "        {\"name\": \"heads\", \"type\": \"choice\", \"values\": [1,2], \"value_type\": \"int\", \"is_ordered\" : True,\"sort_values\":False},\n",
    "        #{\"name\": \"heads\", \"type\": \"choice\", \"values\": [1], \"value_type\": \"int\", \"is_ordered\" : True,\"sort_values\":False},\n",
    "        \n",
    "        #{\"name\": \"aggregation\", \"type\" : \"choice\", \"values\" :[\"sum\", \"mean\", \"max\"], \"value_type\" : \"str\"}\n",
    "        {\"name\": \"aggregation\", \"type\" : \"choice\", \"values\" :[\"max\"], \"value_type\" : \"str\"},\n",
    "     \n",
    "    ],\n",
    "  \n",
    "    evaluation_function=train_evaluate,\n",
    "    objective_name='AVG_total_loss',\n",
    "    arms_per_trial=1,\n",
    "    minimize = True,\n",
    "    random_seed = 123,\n",
    "    total_trials = 20\n",
    ")\n",
    "\n",
    "print(best_parameters)\n",
    "means, covariances = values\n",
    "print(means)\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.service.utils.report_utils import exp_to_df\n",
    "\n",
    "results = exp_to_df(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=\"AVG_total_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values(by=\"AVG_total_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"BPI17O.csv\", sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
